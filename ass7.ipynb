{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b155636-b473-41b6-933c-145d51011bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample text\n",
    "\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog. Dogs are not lazy by nature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2ca9b65-d439-42d8-91b7-6d341532bae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk with important download\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3106b103-195c-48c4-b673-d6bd044c958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e70d00b-6830-40b3-adc9-781d8156c652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'Dogs', 'are', 'not', 'lazy', 'by', 'nature']\n",
      "\n",
      "POS Tags:  [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('Dogs', 'NNS'), ('are', 'VBP'), ('not', 'RB'), ('lazy', 'JJ'), ('by', 'IN'), ('nature', 'NN')]\n",
      "\n",
      " Filtered tokens (StopWords Removed):  ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', '.', 'Dogs', 'lazy', 'nature']\n",
      "\n",
      "Stemmed tokens: ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', '.', 'dog', 'lazi', 'natur']\n",
      "\n",
      "Lemmatized tokens:  ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', '.', 'Dogs', 'lazy', 'nature']\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "\n",
    "\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(\"Tokens: \" , tokens)\n",
    "#pos tagging\n",
    "\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS Tags: \", pos_tags)\n",
    "\n",
    "\n",
    "#Remove stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"\\n Filtered tokens (StopWords Removed): \", filtered_tokens)\n",
    "\n",
    "#stemming\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens  = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nStemmed tokens:\",stemmed_tokens)\n",
    "\n",
    "#lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nLemmatized tokens: \", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e3ad333-ecfd-4834-a255-1205c6adaf86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TF-IDF Matrix:\n",
      "        and   animals       are     brown       dog      dogs       fox  \\\n",
      "0  0.000000  0.000000  0.000000  0.528635  0.000000  0.000000  0.528635   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.528635  0.000000  0.000000   \n",
      "2  0.447214  0.447214  0.447214  0.000000  0.000000  0.447214  0.000000   \n",
      "\n",
      "      foxes      lazy     quick    sleeps      the  \n",
      "0  0.000000  0.000000  0.528635  0.000000  0.40204  \n",
      "1  0.000000  0.528635  0.000000  0.528635  0.40204  \n",
      "2  0.447214  0.000000  0.000000  0.000000  0.00000  \n"
     ]
    }
   ],
   "source": [
    "# term frequency and inverse document frequency(IDF)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# sample corpus(multiple document)\n",
    "\n",
    "corpus = [\n",
    "    \"The quick brown fox.\",\n",
    "    \"The lazy dog sleeps.\",\n",
    "    \"Dogs and foxes are animals.\"\n",
    "]\n",
    "\n",
    "# TF-IDF Vecotorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#fit and transform the corpus\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# convert the tf-idf matrix to a readable format\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense  = tfidf_matrix.todense()\n",
    "df_tfidf = pd.DataFrame(dense,columns = feature_names)\n",
    "\n",
    "print(\"\\n TF-IDF Matrix:\")\n",
    "print(df_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd333b-b123-4527-ad8f-fe5d1585ce77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cade55-9fc3-44b6-a55c-303aefad4f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
